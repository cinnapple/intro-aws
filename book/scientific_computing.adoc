== クラウドで行う科学計算・機械学習

ここからが第二回目の講義の内容になる．

第二回目は，前回学んだクラウドの知識・技術を使って，現実的な問題を解くことを考える．

計算機が発達した現代では，計算機によるシミュレーションやビッグデータの解析は，科学・エンジニアリングの研究の主要な柱である．
これらの大規模な計算を実行するには，クラウドは最適である．
本講義では，どのようにしてクラウド上で科学計算を実行するのかを，ハンズオンとともに体験してもらう．
科学計算の具体的な題材として，今回は機械学習(ディープラーニング)を取り上げる．

なお，本講義では https://pytorch.org/[PyTorch] ライブラリを使ってディープラーニングのアルゴリズムを走らせるが，ディープラーニングおよび PyTorch の知識は不要である．
講義ではなぜ・どうやってディープラーニングをクラウドで実行するか，に主眼を置いているので，実行するプログラムの詳細には立ち入らない．
将来自分でディープラーニングを使う機会が来たときに，詳しく学んでもらいたい．

=== なぜ機械学習をクラウドで行うのか？

2010年代後半に始まったAIブームのおかげで，研究だけでなく社会・ビジネスの文脈でも機械学習に高い関心が寄せられている．
特に，ディープラーニングと呼ばれる多層のレイヤーからなるニューラルネットワークを用いたアルゴリズムは，画像認識や自然言語処理などの分野で圧倒的に高い性能を実現し，革命をもたらしている．

ディープラーニングの特徴は，なんといってもそのパラメータの多さである．
層が深くなるほど，層間のニューロンを結ぶ"重み"パラメータの数が増大していく．
例えば，最新の言語モデルである https://arxiv.org/abs/2005.14165[GPT-3] には**1750億個**ものパラメータが含まれている！
このような膨大なパラメータを有することで，ディープラーニングは高い表現力と汎化性能を実現しているのである．

.ニューラルネットワークにおける畳み込み演算．
image::imgs/neural_network.png[cdk output, 500, align="center"]

GPT-3 に限らず，最近の SOTA (State-of-the-art) の性能を達成するニューラルネットでは，百万から億のオーダーのパラメータを有することは頻繁になってきている．そのような巨大なニューラルネットを学習(トレイン)させるのは，当然のことながら巨大な計算コストがかかる．そんな巨大な計算に最適なのが，クラウドである！事実，GPT-3の学習も，詳細は明かされていないが，Microsoft社のクラウドを使って行われたと報告されている．

[TIP]
====
GPT-3 が発表された時，そのモデルがもつ表現能力には多くの人が驚嘆させられた． https://openai.com/blog/better-language-models/#task5[OpenAI] のブログに，モデルが出力した翻訳や文章要約のタスクの結果が紹介されている．
====

=== GPU による機械学習の高速化

ディープラーニングの計算で欠かすことのできない技術として， **GPU (Graphics Processing Unit)** について少し説明する．

GPUは，その名のとおり，元々はコンピュータグラフィックスを出力するための専用計算チップである．CPU (Central Processing Unit) に対し，グラフィックスの演算に特化した設計がなされている．身近なところでは，XBoxやPS4などのゲーム機などに搭載されているし，ハイスペックなノートパソコンやデスクトップコンピュータにも搭載されていることがある．コンピュータグラフィクスでは，スクリーンにアレイ状に並んだ数百万個の画素をリアルタイムで処理する必要がある．そのため，GPUは，コアあたりの演算能力は比較的弱いかわりに，チップあたり数百から数千のコアを搭載しており (<<gpu_architecture>>)，スクリーンの画素を並列的に処理することで，リアルタイムでの描画を実現している．

[[gpu_architecture]]
.GPUのアーキテクチャ．GPUには数百から数千の独立した計算コアが搭載されている． (画像出典: https://devblogs.nvidia.com/nvidia-turing-architecture-in-depth/)
image::imgs/gpu_architecture.jpg[cdk output, 500, align="center"]

このように，コンピュータグラフィクスの目的で生まれたGPUだが，2010年前後から，その高い並列計算能力をグラフィックス以外の計算(科学計算など)に用いるという流れ (**General-purpose computing on GPU; GPGPU**) が生まれた．GPUのコアは，その設計から，行列の四則演算など，単純かつ規則的な演算が得意であり，そのような演算に対しては数個程度のコアしか持たないCPUに比べて圧倒的に高い計算速度を実現することができる．現在ではGPGPUは分子動力学や気象シミュレーション，そして機械学習など多くの分野で使われている．

ディープラーニングで最も頻繁に起こる演算が，ニューロンの出力を次の層のニューロンに伝える畳み込み (Convolution) 演算である．畳み込み演算は，まさにGPUが得意とする演算であり，CPUではなくGPUを用いることで学習を飛躍的に(数百倍程度！)加速させることができる．

このように GPU は機械学習の計算で欠かせないものであるが，なかなか高価である．例えば，科学計算・機械学習に専用設計されたNVIDIA社の Tesla V100 というチップは，一台で約百万円の価格が設定されている．機械学習を始めるのに，いきなり百万円の投資はなかなか大きい．だが，クラウドを使えば，そのような初期コスト０でGPUを使用することができるのである！

[NOTE]
====
機械学習を行うのに，V100が必ずしも必要というわけではない．
むしろ，研究者などでしばしば行われるのは，コンピュータゲームに使われるグラフィックス用のGPUを買ってきて(NVIDIA GeForceシリーズなど)，それを機械学習に用いる，というアプローチである．
グラフィックス用のいわゆる"コンシューマGPU"は，市場の需要が大きいおかげで，10万円前後の価格で購入することができる．
V100と比べると，コンシューマGPUはコアの数が少なかったり，メモリーが小さかったり，倍数精度の計算が遅かったりなどで劣る点があるが，ディープラーニングの計算は特に問題なく実行することができる．

ローカル環境でディープラーニングの開発を行うのには，コンシューマGPUで十分であると筆者は考える．
プログラムができあがって，ビッグデータの解析や，モデルをさらに大きくしたいときなどに，クラウドは有効だろう．
====

クラウドでGPUを使うには，GPUが搭載された特別なインスタンスタイプ (`P3`, `P2`, `G3`, `G4` など) を選択しなければならない．
<<table_gpu_instances>> に，代表的なGPU搭載のインスタンスタイプを挙げる (執筆時点(2020/06)での情報)．

[[table_gpu_instances]]
[cols="1,1,1,1,1,1,1", options="header"]
.GPUを搭載したEC2インスタンスタイプ
|===
|Instance
|GPUs
|GPU model
|GPU Mem (GiB)
|vCPU
|Mem (GiB)
|Price per hour ($)

|p3.2xlarge
|1
|NVIDIA V100
|16
|8
|61
|3.06

|p3n.16xlarge
|8
|NVIDIA V100
|128
|64
|488
|24.48

|p2.xlarge
|1
|NVIDIA K80
|12
|4
|61
|0.9

|g4dn.xlarge
|1
|NVIDIA T4
|16
|4
|16
|0.526

|===

<<table_gpu_instances>> からわかるとおり，CPUのみのインスタンスと比べると少し高い価格設定になっている．
また，古い世代のGPU (V100に対してのK80) はより安価な価格で提供されている．
GPUの搭載数は1台から最大で8台まで選択することが可能である．

GPUを搭載した一番安いインスタンスタイプは， `g2dn.xlarge` であり，これには廉価かつ省エネルギー設計の NVIDIA T4 が搭載されている．今回のハンズオンでは，このインスタンスを使用して，ディープラーニングの計算を行ってみる．

[NOTE]
====
V100を一台搭載した `p3.2xlarge` の利用料金は一時間あたり $3.06 である．V100が約百万円で売られていることを考えると，約3000時間 (= 124日間)，通算で計算を行った場合に，クラウドを使うよりもV100を自分で買ったほうがお得になる，という計算になる．

(実際には，自前でV100を用意する場合は，V100だけでなく，CPUやネットワーク機器，電気使用料もかかるので，百万円よりもさらにコストがかかる．)
====

[TIP]
====
GPT-3 で使われた計算リソースの詳細は論文でも明かされていないのだが， https://lambdalabs.com/blog/demystifying-gpt-3/[Lambda社のブログ]で興味深い考察が行われている (Lambda社は機械学習に特化したクラウドサービスを提供している)．

記事によると，1750億のパラメータを学習するには，一台のGPU (NVIDIA V100)を用いた場合，342年の月日と460万ドルのクラウド利用料が必要となる，とのことである．GPT-3のチームは，複数のGPUに処理を分散することで現実的な時間のうちに学習を完了させたのであろうが，このレベルのモデルになってくるとクラウド技術の限界を攻めないと達成できないことは確かである．
====

